{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidate data across multiple SQL servers and databases into a single table\n",
    "\n",
    "Contributors: Orvin Bellamy (https://github.com/orvinbellamy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries (some may be redundant)\n",
    "\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from itertools import repeat\n",
    "from sqlalchemy import create_engine, event\n",
    "from urllib.parse import quote_plus\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you store the .env file\n",
    "# Note that .env file should contain server name of production & development database, and userID & password\n",
    "# For example --> production_server = 'xxxx.database.xxxxx.net'\n",
    "# Follow naming conventions: production_server, development_server, userid, password\n",
    "env_path = Path('INSERT PATH')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Store server names in variables\n",
    "production_db = os.getenv('production_server')\n",
    "dev_db = os.getenv('development_server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all processing functions. You can create your own.\n",
    "\n",
    "def composite_key(dframe, new_key, key1, key2):\n",
    "    dframe[new_key] = dframe[key1]+\"-\"+dframe[key2].astype(str)\n",
    "\n",
    "def drop_col(dframe, colnames):\n",
    "    dframe = dframe.drop_duplicates(subset=colnames, keep='first')\n",
    "\n",
    "def split_column(dframe, arg: list):\n",
    "    dframe[arg[0]] = dframe[arg[1]].str.split(',', n=-1, expand=True).loc[:, :(len(arg[0])-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing function. JSON cannot store Python functions, so we store it directly in a Python dictionary.\n",
    "# the JSON file stores the key to the function stored in pp_functions dictionary\n",
    "# main() will call the function based on the key stored in the JSON file --> pp_functions[key stored in JSON]()\n",
    "pp_functions = {\n",
    "    \"composite_key\": composite_key,\n",
    "    \"drop_col\": drop_col,\n",
    "    \"split_column\": split_column\n",
    "}\n",
    "\n",
    "# server information are stored in .env, not JSON. JSON stores the keys to query_db where the server information are stored\n",
    "# similar concept with processing, main() refer to query_db based on the key stored in the JSON file\n",
    "query_db = {\n",
    "    \"production_db\": production_db,\n",
    "    \"dev_db\": dev_db\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to connect to the database. Only arguments needed are server address and database\n",
    "# we can loop through the list of database name and use this function to extract all their data\n",
    "def connect(server, database):\n",
    "\n",
    "    # set global variables. We need cur (cursor) to perform SQL queries\n",
    "    global cnxn_str, cnxn, cur, quoted, engine\n",
    "\n",
    "    # may need to change driver depending on your server\n",
    "    cnxn_str = (\"Driver={SQL Server Native Client 11.0};\"\n",
    "                \"Server=\" + server + \";\"\n",
    "                \"Database=\" + database + \";\"\n",
    "                \"UID=\" + os.getenv('userid') + \";\"\n",
    "                \"PWD=\" + os.getenv('password') + \";\")\n",
    "\n",
    "    # connect to the database\n",
    "    cnxn = pyodbc.connect(cnxn_str)\n",
    "    \n",
    "    cur = cnxn.cursor()\n",
    "    cur.fast_executemany=True\n",
    "\n",
    "    quoted = quote_plus(cnxn_str)\n",
    "\n",
    "    # engine is used for INSERT INTO function by pandas libraries (see later)\n",
    "    engine = create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quoted), fast_executemany=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for privacy/security purposes, I have stored all the names of the database in a table on SQL server\n",
    "# this function queries all the database names from the table\n",
    "# if you have the list of all the databases you want to loop through, ignore this and set the db_name_list variable manually\n",
    "def get_db_name():\n",
    "\n",
    "    connect(dev_db, 'database_name')\n",
    "\n",
    "    cur.execute(\"SELECT [db_name] FROM table_where_i_store_db_names\")\n",
    "\n",
    "    results = cur.fetchall()\n",
    "\n",
    "    results = [i[0] for i in results]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of database names. We will use this to loop queries for each database\n",
    "db_name_list = get_db_name()\n",
    "\n",
    "# get the JSON file\n",
    "with open('operations.json', 'r') as json_file:\n",
    "    operations = json.loads(json_file.read())\n",
    "\n",
    "    print(\"type: \", type(operations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main() function executes all the steps\n",
    "# 1. connect to the server\n",
    "# 2. loop through each databases and query the same table\n",
    "# 3. combine all query results into one dataframe\n",
    "# 4. perform all preprocessing as defined in the JSON file\n",
    "# 5. update (delete and insert) the consolidated data into a single table on SQL server\n",
    "\n",
    "def main(op_key):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print('Operation: ' + op_key)\n",
    "\n",
    "    # define global variables df (dataframe) and tb (table)\n",
    "    # tb is a numpy array in (M x N+1) shape where M is the number of columns and N is the number of rows\n",
    "    # first row of tb is always the column name\n",
    "    global df, tb, results\n",
    "\n",
    "    # set first row of tb as column name\n",
    "    tb = np.array([operations[op_key]['header']], dtype=\"object\")\n",
    "\n",
    "    # set row_tracker as a list that tracks the database\n",
    "    # the first item is set as 'db' because this will be the column header when added to df (dataframe)\n",
    "    # for example, if the first 10 rows extracted are from Abilene, then item 2 to 11 in row_tracker will be 'Abilene'\n",
    "    row_tracker = ['db']\n",
    "\n",
    "    # loop through each database from eadbpool (list of database names)\n",
    "    print('Querying Data...')\n",
    "\n",
    "    # if query from production database, execute code below\n",
    "    if query_db[operations[op_key][\"query_db\"]] == production_db:\n",
    "        for db_name in db_name_list:\n",
    "            \n",
    "            # connect to the database\n",
    "            try:\n",
    "                connect(production_db, db_name)\n",
    "            except:\n",
    "                try:\n",
    "                    connect(dev_db, db_name)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            cur.rollback()\n",
    "\n",
    "            # execute the query according to the data we want to get from operations dictionary\n",
    "            cur.execute(operations[op_key]['query'])\n",
    "            \n",
    "            # store query result in a temporary variable\n",
    "            results = cur.fetchall()\n",
    "\n",
    "            # print the database name and the number of rows extracted\n",
    "            print(db_name + \" \" + str(len(results)))\n",
    "\n",
    "            # add database name to row_tracker for each row queried\n",
    "            row_tracker.extend([db_name] * len(results))\n",
    "\n",
    "            # add queried data into the numpy array\n",
    "            try:\n",
    "                tb = np.insert(tb, tb.shape[0], results, axis=0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # add the row_tracker as a new column in the numpy array\n",
    "        tb = np.insert(tb, tb.shape[1], row_tracker, axis=1)\n",
    "    \n",
    "    # if query from Eproval-Company, execute code below\n",
    "    elif query_db[operations[op_key][\"query_db\"]] == dev_db:\n",
    "        connect(dev_db, 'Eproval-Company')\n",
    "\n",
    "        cur.rollback()\n",
    "\n",
    "        cur.execute(operations[op_key]['query'])\n",
    "        results = cur.fetchall()\n",
    "        tb = np.insert(tb, tb.shape[0], results, axis=0)\n",
    "\n",
    "\n",
    "    print('Querying done!')\n",
    "    print('Data processing...')\n",
    "\n",
    "    # transform numpy array to dataframe\n",
    "    df = pd.DataFrame(tb[1:,],columns=tb[0])\n",
    "\n",
    "    # for each list of arguments set, execute preprocessing function\n",
    "    # this will mostly be creating composite keys\n",
    "    try:\n",
    "        for arguments in operations[op_key]['preprocess']['arguments']:\n",
    "            pp_functions[operations[op_key]['preprocess']['functions'][0]](df, *arguments)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # using convert_dic dictionary, convert each column to the appropriate data type\n",
    "    # df = df.astype(operations[op_key]['convert_dic'])\n",
    "\n",
    "    # set chunk number, this is to set how much data is uploaded (INSERT INTO statement) to SQL server at once\n",
    "    # more columns means fewer rows per chunks\n",
    "    # maximum chunk is 2100, but is set to 2000 to avoid error. This is limitation of SQL server, do not change\n",
    "    chunknum = math.floor(2000/len(df.columns))\n",
    "    chunknum = 1000 if chunknum > 1000 else chunknum\n",
    "\n",
    "    # connect to Eproval-Company\n",
    "    print('Connecting to Eproval-Company')\n",
    "    connect(dev_db, 'Eproval-Company')\n",
    "\n",
    "    # reset cursor\n",
    "    cur.rollback()\n",
    "\n",
    "    # delete old data from the table in Eproval-Company\n",
    "    print('Deleting old data in table...')\n",
    "    cur.execute(\"DELETE FROM [dbo].[\" + operations[op_key]['table_name'] + \"]\")\n",
    "    # commit the query\n",
    "    cur.commit()\n",
    "\n",
    "    # upload (INSERT INTO statement) data from dataframe to the table in SQL server\n",
    "    # this is a function from pandas library. Do not change arguments, it's already optimized for maximum speed\n",
    "    print('Uploading new data (INSERT INTO)...')\n",
    "    df.to_sql(operations[op_key]['table_name'], engine, index=False, \\\n",
    "        if_exists='append', schema='dbo', chunksize=chunknum, method='multi')\n",
    "    \n",
    "    # error handling. If there is any ALTER query required after upload, execute now\n",
    "    # mostly for correcting DATE data type in SQL server\n",
    "    for query in operations[op_key]['alter_query']:\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            cur.commit()\n",
    "        except:\n",
    "            cur.rollback()\n",
    "    cnxn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    cnxn.close\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation: aspnet_users\n",
      "Querying Data...\n",
      "Columbus 1\n",
      "Savannah 1\n",
      "FortCollins 1\n",
      "SanDiego 1\n",
      "Albuquerque 1\n",
      "Guelph 1\n",
      "Kelowna 1\n",
      "Dallas 1\n",
      "Irvine 1\n",
      "Aurora-il 1\n",
      "Huntsville 1\n",
      "Denver 1\n",
      "Boulder 1\n",
      "Barrie 1\n",
      "Norwalk 1\n",
      "Sturgeon 1\n",
      "Gainesville 1\n",
      "Chattanooga 1\n",
      "Abilene 1\n",
      "Newark 1\n",
      "Querying done!\n",
      "Data processing...\n",
      "Connecting to Eproval-Company\n",
      "Deleting old data in table...\n",
      "Uploading new data (INSERT INTO)...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# loop through each key of operations and execute main() function\n",
    "# basically get all data\n",
    "\n",
    "for keys in operations:\n",
    "    main(keys)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad1682ba5c7025313bc9efa139e666b9bab29454e80d13a6dc4d7f2be864baa3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
